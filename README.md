# rvqa

The Relation-VQA dataset is released!

If you have any question, please do not hesitate to contact me.


## Relation-VQA Dataset



**The whole dataset**
- [vqa-rel_map_result_0.30.json](https://drive.google.com/open?id=1OnzMi23jVAyXQP7xmEBweg2BrhJm4t7D)
- [vqa-rel_map_result_0.30.txt](https://drive.google.com/open?id=1_nuNvYRdpRYvU39CH3Kaqcz1CyzKUDJj)

**The splitting dataset**
- [vqa_raw_train_0.30.json](https://drive.google.com/open?id=1w1Kaqfsn1qKII9d6Al18xsr7Bj2blCl8)
- [vqa_raw_val_0.30.json](https://drive.google.com/open?id=1GnID5XgWav2ao6J8cswwtWPx9lKzML3z)
- [vqa_raw_train_val_0.30.json](https://drive.google.com/open?id=1LNc6kX1gRlFyJNwhtuDxj4f1WEeYmqB-)
- [vqa_raw_test_0.30.json](https://drive.google.com/open?id=1j5YlmsQW5yfyZHZr8kUN8ZC_ufOCvZwe)

**Alias rules**
- [alias_map_dict.json](https://drive.google.com/open?id=1_muDDtiYNX3nR3cZ8fz7ivZaVcTTgT2a)

### Reference
- **Paper on SIGKDD**: http://www.kdd.org/kdd2018/accepted-papers/view/r-vqa-learning-visual-relation-facts-with-semantic-attention-for-visual-que
- **Paper on arXiv**: https://arxiv.org/abs/1805.09701



If you use this project as part of any published research, please acknowledge the following paper.
```
@inproceedings{lu2018rvqa,
	title={R-VQA: Learning Visual Relation Facts with Semantic Attention for Visual Question Answering.},
	author={Lu, Pan and Ji, Lei and Zhang, Wei and Duan, Nan and Zhou, Ming and Wang, Jianyong},
	booktitle={SIGKDD 2018},
	year={2018}
}
```
